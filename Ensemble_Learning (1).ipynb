{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "- Ensemble learning is a technique in machine learning where multiple models (often called weak learners) are combined to form a single, more powerful model (strong learner).                               \n",
        "- Key idea behind it are :\n",
        " Instead of relying on one model, we combine predictions from multiple models to:\n",
        "  - Reduce variance (Bagging)\n",
        "\n",
        "  - Reduce bias (Boosting)\n",
        "\n",
        "  - Improve generalization and stability"
      ],
      "metadata": {
        "id": "9q5BsoEugpml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Bagging and Boosting?\n",
        "- Bagging (Bootstrap Aggregating)\n",
        "\n",
        "  - Trains multiple models independently on different bootstrap samples.\n",
        "\n",
        "  - Predictions are combined using majority vote (classification) or average (regression).\n",
        "\n",
        "  - Main goal: Reduce variance and improve stability.\n",
        "\n",
        "  - Example: Random Forest.\n",
        "\n",
        "- Boosting\n",
        "\n",
        "  - Trains models sequentially, where each new model focuses on the errors of the previous one.\n",
        "\n",
        "  - Predictions are combined in a weighted manner.\n",
        "\n",
        "  - Main goal: Reduce bias and make the model more accurate.\n",
        "\n",
        "  - Examples: AdaBoost, Gradient Boosting, XGBoost."
      ],
      "metadata": {
        "id": "D49Ja-HFhe91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "- Bootstrap Sampling = Random sampling with replacement from the training data to create multiple subsets.\n",
        "\n",
        "  Each subset has the same size as the original dataset, but due to replacement, some samples appear multiple times and some not at all.\n",
        "\n",
        "- Role in Bagging:\n",
        "\n",
        "  - Ensures diversity in training subsets.\n",
        "\n",
        "  - Each base learner trains on different subsets, reducing variance.\n",
        "\n",
        "  - Used in Random Forest for training multiple decision trees."
      ],
      "metadata": {
        "id": "G5Rdvntahfm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "- Since bootstrap samples don’t include all original data, the data not included is called Out-of-Bag (OOB) samples.\n",
        "\n",
        "- OOB samples act like a validation set to estimate performance without cross-validation.\n",
        "\n",
        "- OOB Score: The average prediction accuracy on OOB samples → useful for model evaluation in Bagging/Random Forest."
      ],
      "metadata": {
        "id": "GtuctPZ1jTiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a Random forest.\n",
        "- Single Decision Tree: Importance is based on information gain (Gini/Entropy) from splits.\n",
        "\n",
        "  - Can be biased toward features with many categories.\n",
        "\n",
        "- Random Forest: Aggregates feature importance across many trees.\n",
        "\n",
        "  - More stable, robust, and less biased.\n",
        "\n",
        "  - Provides a better global ranking of features.\n"
      ],
      "metadata": {
        "id": "6_kCNiC3jvYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to:● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() ● Train a Random Forest Classifier\n",
        "#● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIuqBm1NjJ9A",
        "outputId": "b20efa38-ad8a-4fe5-fb0a-2ae8c72ada89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to: ● Train a Bagging Classifier using Decision Trees on the Iris dataset ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "\n",
        "# Bagging Classifier\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred_bag))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzBwoRGgjKlQ",
        "outputId": "37513ad7-7f85-46de-a439-25314e1b7626"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to: ● Train a Random Forest Classifier ● Tune hyperparameters max_depth and n_estimators using GridSearchCV ● Print the best parameters and final accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 5, 10, 20]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate\n",
        "best_rf = grid.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjwaaq6CjKYz",
        "outputId": "c51cd62c-1007-4889-d217-d611cb9c34ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to: ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset ● Compare their Mean Squared Errors (MSE)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bagging = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", mse_bag)\n",
        "print(\"Random Forest Regressor MSE:\", mse_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtNcPa5dlZcP",
        "outputId": "21339f87-5d22-4579-ee9d-c302a110c5c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25787382250585034\n",
            "Random Forest Regressor MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "  \n",
        "    You decide to use ensemble techniques to increase model performance.\n",
        "  \n",
        "    Explain your step-by-step approach to: ● Choose between Bagging or Boosting ● Handle overfitting ● Select base models ● Evaluate performance using cross-validation ● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "-  Step-by-Step Approach:\n",
        "\n",
        "1. Choose between Bagging or Boosting\n",
        "\n",
        "   - If data is noisy → Bagging (Random Forest)\n",
        "\n",
        "   - If we need higher accuracy and bias reduction → Boosting (XGBoost/LightGBM)\n",
        "\n",
        "   - Loan defaults often need catching minority cases → Boosting is preferred.\n",
        "\n",
        "2. Handle Overfitting\n",
        "\n",
        "   - Use cross-validation\n",
        "\n",
        "   - Limit tree depth, learning rate (Boosting)\n",
        "\n",
        "   - Use regularization\n",
        "\n",
        "3. Select Base Models\n",
        "\n",
        "   - Decision Trees (commonly used as weak learners)\n",
        "\n",
        "   - Can also try Logistic Regression or SVM with Bagging\n",
        "\n",
        "4. Evaluate Performance\n",
        "\n",
        "   - Use cross-validation with metrics like AUC-ROC, Precision, Recall, F1-score (since data may be imbalanced).\n",
        "\n",
        "   - OOB error for Random Forest.\n",
        "\n",
        "5. Justification for Ensemble Learning\n",
        "\n",
        "   - Captures complex non-linear patterns.\n",
        "\n",
        "   - Reduces risk of missing potential defaults.\n",
        "\n",
        "   - Provides more stable predictions than a single model.\n",
        "\n",
        "   - Improves decision-making → lower loan default risk → better financial outcomes."
      ],
      "metadata": {
        "id": "bVj8uhVPloM8"
      }
    }
  ]
}